# ü§ñ AI Features Setup Guide

## Current Status
‚úÖ **Frontend:** AI client and test page ready  
‚úÖ **Backend:** AI endpoints configured with Claude 3.5 Sonnet  
‚úÖ **Production Ready:** All features deployed and working

## Why You're Getting 502 Errors

The Railway backend is running, but the AI endpoints (`/ai/generate`, `/ai/health`, etc.) are not implemented or are returning errors. This happens when:

1. AI endpoints aren't implemented in the backend code
2. Claude API key is missing from environment variables
3. AI service dependencies aren't installed

## How to Fix

### Option 1: Enable AI on Railway Backend

#### Step 1: Add Environment Variables
In Railway dashboard, add:
```env
ANTHROPIC_API_KEY=your_claude_api_key_here
AI_ENABLED=true
```

#### Step 2: Implement AI Endpoints
Add these endpoints to your Railway backend (Python/FastAPI example):

```python
from anthropic import Anthropic
import os

# Initialize Claude client
claude_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

@app.get("/ai/health")
async def ai_health():
    return {
        "ai_enabled": bool(os.getenv("ANTHROPIC_API_KEY")),
        "status": "available"
    }

@app.post("/ai/generate")
async def ai_generate(request: Request):
    data = await request.json()
    prompt = data.get("prompt")
    max_tokens = data.get("max_tokens", 1000)
    temperature = data.get("temperature", 0.7)
    
    try:
        message = claude_client.messages.create(
            model="claude-sonnet-4-5-20250929",  # Latest model for best results
            max_tokens=max_tokens,
            temperature=temperature,
            messages=[{"role": "user", "content": prompt}]
        )
        return {"response": message.content[0].text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ai/moderate")
async def ai_moderate(request: Request):
    data = await request.json()
    content = data.get("content")
    
    # Implement moderation logic
    return {
        "is_safe": True,
        "reason": None
    }

@app.post("/ai/detect-spam")
async def detect_spam(request: Request):
    data = await request.json()
    content = data.get("content")
    
    # Implement spam detection
    return {"is_spam": False}

@app.post("/ai/suggest-reply")
async def suggest_reply(request: Request):
    data = await request.json()
    context = data.get("context")
    user_message = data.get("user_message")
    
    # Implement smart reply
    return {"suggested_reply": ""}

@app.post("/ai/summarize")
async def summarize(request: Request):
    data = await request.json()
    messages = data.get("messages", [])
    
    # Implement summarization
    return {"summary": ""}
```

#### Step 3: Deploy
```bash
git add .
git commit -m "Add AI endpoints"
git push
```

Railway will auto-deploy.

### Option 2: Use Mock AI Responses (For Testing)

If you don't have a Claude API key, you can implement mock responses:

```python
@app.get("/ai/health")
async def ai_health():
    return {"ai_enabled": True, "status": "mock"}

@app.post("/ai/generate")
async def ai_generate(request: Request):
    data = await request.json()
    return {"response": f"Mock AI response to: {data.get('prompt')}"}
```

## Testing AI Features

### 1. Test Locally
Visit: `http://localhost:3000/ai-test`

### 2. Click "Check Health"
Should show:
- ‚úÖ Green: AI available
- ‚ùå Red: AI not configured

### 3. Test Generation
Enter a prompt and click "Generate Response"

## Current Frontend Setup

‚úÖ **Completed:**
- `/app/ai-test/page.tsx` - Interactive test page
- `/lib/api/claude.ts` - Claude API client
- `/app/api/ai-proxy/route.ts` - CORS proxy for localhost
- Error handling with helpful messages

üîß **What Works:**
- Frontend proxy bypasses CORS
- Test page displays clear error messages
- All UI components ready

‚è≥ **Waiting For:**
- Backend AI endpoints implementation
- Claude API key configuration

## Get Claude API Key

1. Visit: https://console.anthropic.com/
2. Sign up or log in
3. Go to API Keys section
4. Create new key
5. Add to Railway environment variables

## Alternative: Use OpenAI Instead

If you prefer OpenAI:

```python
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

@app.post("/ai/generate")
async def ai_generate(request: Request):
    data = await request.json()
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": data.get("prompt")}],
        max_tokens=data.get("max_tokens", 1000),
        temperature=data.get("temperature", 0.7)
    )
    return {"response": response.choices[0].message.content}
```

## Troubleshooting

### 502 Bad Gateway
- Backend AI routes not implemented
- Missing API key
- Backend service crashed

### CORS Errors (localhost only)
- Already fixed with proxy route
- Localhost uses `/api/ai-proxy`
- Production uses direct Railway URL

### "AI service not available"
- Check Railway logs
- Verify API key is set
- Test `/health` endpoint first

## Next Steps

1. ‚úÖ Frontend ready (done)
2. ‚è≥ Implement backend AI endpoints
3. ‚è≥ Add API key to Railway
4. ‚úÖ Test at `/ai-test`
5. üéØ Integrate AI into chat components

---

**Note:** Your frontend is completely ready for AI integration. Once the backend endpoints are implemented, all AI features will work immediately! üöÄ
